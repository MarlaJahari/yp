{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ok_ERAjejD2s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading dataset\n",
        "train= dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(),download=True)\n",
        "test= dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())"
      ],
      "metadata": {
        "id": "TUC0grYDjTp4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#specifying epoches, no: of samples and iterations\n",
        "n=5000\n",
        "nsamples=90\n",
        "epochs = int(n/(len(train)/nsamples))\n",
        "val_test= torch.utils.data.DataLoader(dataset=test,batch_size=nsamples,shuffle=False)\n",
        "val_train= torch.utils.data.DataLoader(dataset=train,batch_size=nsamples,shuffle=True)"
      ],
      "metadata": {
        "id": "iQpfGZGujacp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A default model class where the weight matrix relating first and second hidden layers are not compressed**"
      ],
      "metadata": {
        "id": "C1PWzOZAXLQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#constructing model class 1\n",
        "#in which the weight matrix relating hidden layers are not compressed\n",
        "\n",
        "class MLPerceptronNNModel(nn.Module):\n",
        "    def __init__(self, input, hidden, output):\n",
        "        super(MLPerceptronNNModel, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input, hidden),  #1st linear function\n",
        "            nn.ReLU(),                           #1-N.Linear\n",
        "            nn.Linear(hidden, hidden), #2nd linear function\n",
        "            nn.ReLU(),                           #2-N.Linear\n",
        "            nn.Linear(hidden, output)  #final lin function\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "s1CaR79ojgBa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A model class where the weight matrix relating first and second hidden layers are compressed by the low-rank factorization algorithm SVD**"
      ],
      "metadata": {
        "id": "gmi6nF9EXjY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class CompressedMLPerceptronNNModel(nn.Module):\n",
        "    def __init__(self, input, hidden, output, rank):\n",
        "        super(CompressedMLPerceptronNNModel, self).__init__()\n",
        "        self.rank= rank\n",
        "\n",
        "        #1st linear function,\n",
        "        #28*28->1000\n",
        "        self.fc1= nn.Linear(input, hidden)\n",
        "        self.relu1= nn.ReLU() # Non-linear 1\n",
        "\n",
        "        #extracting the weight matrix of self.fc2\n",
        "        self.fc2_weight = nn.Parameter(torch.Tensor(hidden, hidden))\n",
        "        nn.init.xavier_uniform_(self.fc2_weight)  # initializing the weight matrix\n",
        "\n",
        "        #performing SVD\n",
        "        u,s,vt = torch.svd(self.fc2_weight)\n",
        "\n",
        "        #truncating singular values to get desired rank\n",
        "        u= u[:, :rank]\n",
        "        s= torch.diag(s[:rank])\n",
        "        vt= vt[:rank, :]\n",
        "\n",
        "        #reconstructing the compressed weight matrix\n",
        "        self.fc2_compressed = nn.Parameter(torch.matmul(torch.matmul(u, s), vt))\n",
        "\n",
        "        #non-linearity 2\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        #lin function 3 (output layer):\n",
        "        #1000->digits(10)\n",
        "        self.fc3 = nn.Linear(hidden,output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #1st function\n",
        "        out=self.relu1(self.fc1(x))\n",
        "\n",
        "        #compressed 2nd function\n",
        "        out=torch.matmul(out, self.fc2_compressed)\n",
        "        out=self.relu2(out)\n",
        "\n",
        "        #3rd function (output)\n",
        "        out=self.fc3(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "3D3AE5s4vt2n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A model class where the weight matrix relating first and second hidden layers are compressed using Randomized SVD introduced in \"Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions\" by Halko Et al.**"
      ],
      "metadata": {
        "id": "wXxLQgu9X3R1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from numpy import random, linalg\n",
        "\n",
        "#RANDOMIZED RSVD TO FOR COMPRESSING NN HIDDEN LAYER WEIGHT MATRIX\n",
        "#inputs: derired rank approximation, sampling param for gaussian approximation, no: of power iterations\n",
        "#return: if K~ U*S*Vt, return U, S, Vt\n",
        "def randsvd(K, rank, param_oversampling=None, n_power_iter=None,  return_range=False):\n",
        "\n",
        "    m,n = K.shape\n",
        "    n_samples= 2*rank\n",
        "\n",
        "    #Pt 1: randomized range finder\n",
        "    Q = find_range(K, n_samples, n_power_iter)\n",
        "\n",
        "    #Pt 2: performing required rsvd\n",
        "    B = np.matmul(Q.T, K)\n",
        "    U_tilde, S, Vt = np.linalg.svd(B)\n",
        "    U = np.matmul(Q, U_tilde)\n",
        "\n",
        "    #truncating to desired rank\n",
        "    U,S,Vt = U[:, :rank], S[:rank], Vt[:rank, :]\n",
        "\n",
        "    if return_range:\n",
        "        return U,S,Vt, Q\n",
        "    return U,S,Vt\n",
        "\n",
        "#RANDOMIZED RANGE FINDER FOR RSVD TO COMPUTE AN ORTHONORMAL MATRIX APPROXIMATING THE RANGE OF A\n",
        "#inputs: Matrix K, no: of random gaussian samples, no:of iterations\n",
        "def find_range(K, n_samples, n_iters=None):\n",
        "    m, n = K.shape\n",
        "    O = np.random.randn(n, n_samples)\n",
        "    Y = np.matmul(K, O)\n",
        "\n",
        "    if n_iters:\n",
        "        return sub_iter(K, Y, n_iters)\n",
        "    else:\n",
        "        return OB(Y)\n",
        "\n",
        "#RANDOMIZED SUBSPACE ITERATION FOR AN APPROXIMATE RANGE OF A FROM POWER ITERATIONS\n",
        "#input: K(m*n), some init range of K, no: of iterations\n",
        "#return: approximate range of K from power iterations\n",
        "def sub_iter(A, Y0, n_iters):\n",
        "    Q = OB(Y0)\n",
        "    for i in range(n_iters):\n",
        "        Z=OB(np.matmul(A.T, Q))\n",
        "        Q=OB(np.matmul(A, Z))\n",
        "    return Q\n",
        "\n",
        "\n",
        "#FUNCTION TO COMPUTE ORTHONOMAL BASIS FOR A MATRIX\n",
        "#input: a matrix K (m*n)\n",
        "#output: orthonormal basis for M\n",
        "def OB(K):\n",
        "    Q, c = np.linalg.qr(K)\n",
        "    return Q\n",
        "\n",
        "\n",
        "\n",
        "class CompressedMLPerceptronNNModel2(nn.Module):\n",
        "    def __init__(self, input, hidden, output, rank):\n",
        "        super(CompressedMLPerceptronNNModel2, self).__init__()\n",
        "        self.rank = rank\n",
        "\n",
        "        #1st function\n",
        "        #28*28-> 1000\n",
        "        self.fc1 = nn.Linear(input, hidden)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        #compressed linear function 2\n",
        "        #1000-> 1000\n",
        "        self.fc2_weight = nn.Parameter(torch.Tensor(hidden, hidden))\n",
        "        nn.init.xavier_uniform_(self.fc2_weight)  #initializing the weight matrix\n",
        "        self.fc2_compressed = self.compute_compressed_weight(self.fc2_weight, rank)\n",
        "\n",
        "        #2nd func non-linear pt.\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        #function 3\n",
        "        #1000-> 10\n",
        "        self.fc3 = nn.Linear(hidden, output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #function 1 (lin and non-linear pt)\n",
        "        out = self.relu1(self.fc1(x))\n",
        "\n",
        "        #compressed linear function 2\n",
        "        out = torch.matmul(out, self.fc2_compressed)\n",
        "        #non-linear 2\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        #linear function 3 (output)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "    def compute_compressed_weight(self, weight, rank):\n",
        "\n",
        "        #we convert weight matrix to a np array\n",
        "        weight_np = weight.detach().numpy()\n",
        "\n",
        "        #performing randomized SVD\n",
        "        U,S,Vt = randsvd(weight_np, rank)\n",
        "\n",
        "        #truncate to given rank\n",
        "        U_truncated= U[:, :rank]\n",
        "        S_truncated= S[:rank]\n",
        "        Vt_truncated= Vt[:rank, :]\n",
        "\n",
        "        #reconstructing the compressed weight matrix\n",
        "        compressed_weight=np.dot(U_truncated, np.dot(np.diag(S_truncated), Vt_truncated))\n",
        "\n",
        "        #converting back to torch tensor\n",
        "        compressed_weight=torch.tensor(compressed_weight, dtype=torch.float32)\n",
        "\n",
        "        return nn.Parameter(compressed_weight)\n",
        "\n"
      ],
      "metadata": {
        "id": "UtayUHBPIhBf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instantiating all three models**"
      ],
      "metadata": {
        "id": "p1_ejZeCdftJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiating models\n",
        "\n",
        "model1=MLPerceptronNNModel(input=784, hidden=1000, output=10)\n",
        "model2= CompressedMLPerceptronNNModel(input=784, hidden=1000, output=10, rank=500)\n",
        "model3= CompressedMLPerceptronNNModel2(input=784, hidden=1000, output=10, rank=500)\n",
        "\n"
      ],
      "metadata": {
        "id": "6Gw8KDOFjtT-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instantiating loss and optimizer classes and training the model**"
      ],
      "metadata": {
        "id": "T4ww-9oIaaio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "\n",
        "def train_model(model, learning_rate):\n",
        " optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        " j = 0\n",
        " for epoch in range(epochs):\n",
        "\n",
        "    for i, (images,labels) in enumerate(val_train):\n",
        "\n",
        "        images= Variable(images.view(-1, 28*28))       #converting images to var\n",
        "        labels= Variable(labels)\n",
        "        optimizer.zero_grad()                          #clearing gradients w.r.t. parameters\n",
        "        outputs=model(images)                          #Forward pass to get output\n",
        "        loss = criterion(outputs, labels)              #calculating Loss: softmax -> cross entropy loss\n",
        "        loss.backward()                                #getting gradients w.r.t. parameters\n",
        "        optimizer.step()                               #updating the params\n",
        "\n",
        "\n",
        "        j+=1\n",
        "\n",
        "        if j%500==0:\n",
        "\n",
        "            #calculating accuarcy\n",
        "\n",
        "            right=0\n",
        "            total=0\n",
        "\n",
        "            #iterating through test dataset\n",
        "            for images, labels in val_test:\n",
        "                #Load images to a Torch Variable\n",
        "                images = Variable(images.view(-1, 28*28))\n",
        "\n",
        "                #forward pass only to get output\n",
        "                outputs = model(images)\n",
        "\n",
        "                #Getting predictions from the maximum value\n",
        "                b, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                #total number of labels\n",
        "                total+= labels.size(0)\n",
        "\n",
        "                #correct predictions\n",
        "                right += torch.eq(predicted, labels).sum().item()\n",
        "\n",
        "\n",
        "            acc=100*(right/total)\n",
        "\n",
        "            #LOSS\n",
        "            print('No: of iterations: {}. Loss: {}. Accuracy: {}%'.format(j, loss.item(), acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "r0XuIx_ik33G"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the first model without compression at a learning rate of 0.25**\n",
        "\n"
      ],
      "metadata": {
        "id": "3F6rPG7rgGX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model1, 0.25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qU1aDTuhIdcm",
        "outputId": "5b2118db-2963-4f07-8856-3f8d318e870b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No: of iterations: 500. Loss: 0.0020182463340461254. Accuracy: 98.37%\n",
            "No: of iterations: 1000. Loss: 0.00025715676019899547. Accuracy: 98.4%\n",
            "No: of iterations: 1500. Loss: 0.0002025912399403751. Accuracy: 98.41%\n",
            "No: of iterations: 2000. Loss: 0.0008019665838219225. Accuracy: 98.46000000000001%\n",
            "No: of iterations: 2500. Loss: 0.0014233395922929049. Accuracy: 98.45%\n",
            "No: of iterations: 3000. Loss: 0.0005694674327969551. Accuracy: 98.41%\n",
            "No: of iterations: 3500. Loss: 0.00014881686365697533. Accuracy: 98.34%\n",
            "No: of iterations: 4000. Loss: 0.000434714398579672. Accuracy: 98.41%\n",
            "No: of iterations: 4500. Loss: 0.0006615446181967854. Accuracy: 98.44000000000001%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the second model with the 1000*1000 weight matrix compressed using naive SVD, at a learning rate of 0.25**\n"
      ],
      "metadata": {
        "id": "gYUIhICPkzEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model2, 0.25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qiBw0PqyjARj",
        "outputId": "bcffaf8b-8649-4af4-efbd-4d47e0d1ae57"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No: of iterations: 500. Loss: 0.17118841409683228. Accuracy: 94.77%\n",
            "No: of iterations: 1000. Loss: 0.057777728885412216. Accuracy: 96.78%\n",
            "No: of iterations: 1500. Loss: 0.046327266842126846. Accuracy: 97.49%\n",
            "No: of iterations: 2000. Loss: 0.0688701719045639. Accuracy: 97.39%\n",
            "No: of iterations: 2500. Loss: 0.04349192976951599. Accuracy: 97.82%\n",
            "No: of iterations: 3000. Loss: 0.032681211829185486. Accuracy: 97.76%\n",
            "No: of iterations: 3500. Loss: 0.009368539787828922. Accuracy: 97.88%\n",
            "No: of iterations: 4000. Loss: 0.04232028126716614. Accuracy: 98.19%\n",
            "No: of iterations: 4500. Loss: 0.010601863265037537. Accuracy: 97.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the third model with the 1000*1000 weight matrix compressed using randomized SVD, at a learning rate of 0.25**\n"
      ],
      "metadata": {
        "id": "xel_42RllGoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model3, 0.25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tzIluztcjzjC",
        "outputId": "81faf4af-4650-4839-86dc-4a303ce644e2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No: of iterations: 500. Loss: 0.13663215935230255. Accuracy: 95.15%\n",
            "No: of iterations: 1000. Loss: 0.17118017375469208. Accuracy: 96.94%\n",
            "No: of iterations: 1500. Loss: 0.10826201736927032. Accuracy: 97.6%\n",
            "No: of iterations: 2000. Loss: 0.1346370428800583. Accuracy: 97.69%\n",
            "No: of iterations: 2500. Loss: 0.03268709406256676. Accuracy: 97.61%\n",
            "No: of iterations: 3000. Loss: 0.04211420938372612. Accuracy: 97.88%\n",
            "No: of iterations: 3500. Loss: 0.01729210466146469. Accuracy: 98.15%\n",
            "No: of iterations: 4000. Loss: 0.03451615199446678. Accuracy: 97.98%\n",
            "No: of iterations: 4500. Loss: 0.017224203795194626. Accuracy: 98.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**COMMENTS ON PERFORMANCE**\n",
        "\n",
        "### Model 1 (Non-compressed):\n",
        "- **Loss**: We see the loss decreasing steadily over iterations, so the model is learning effectively.\n",
        "- **Accuracy**: The accuracy is always around 98%, hinting strong performance on the test dataset.\n",
        "\n",
        "### Model 2 (Compressed using Naive SVD):\n",
        "- **Loss**: The loss starts higher compared to the first model but decreases over iterations, although with some fluctuations.\n",
        "- **Accuracy**: Accuracy starts lower compared to Model 1, but it improves steadily over iterations.\n",
        "- **Comparison**: Model 2 has slightly lower accuracy compared to Model 1, probably from loss of performance due to compression. However, the relatively high accuracy implies successful compression.\n",
        "\n",
        "### Model 3 (Compressed using Randomized SVD):\n",
        "- **Loss**: The loss starts relatively high and fluctuates over iterations but generally decreases over time.\n",
        "- **Accuracy**: Similar to Model 2, the accuracy starts lower compared to Model 1 but improves over iterations (but with some fluctuations).\n",
        "- **Comparison**: Model 3 is similar to Model 2 in terms of loss and accuracy. But it achieves slightly higher accuracy compared to Model 2, so the randomized SVD might be a slightly better compression technique for this network.\n"
      ],
      "metadata": {
        "id": "sFCTfYYNl_f-"
      }
    }
  ]
}